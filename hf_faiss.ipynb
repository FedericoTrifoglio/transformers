{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS on GitHub issues using Transformers\n",
    "\n",
    "Goal: implement algo that recommends possible solutions a user's issue\n",
    "\n",
    "Methodology: FAISS (Facebook AI Semantic Search) on embeddings from Transformer model fine-tuned on GitHub issues dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by: https://huggingface.co/course/chapter5/6?fw=tf\n",
    "\n",
    "<p><a href=\"https://colab.research.google.com/drive/1i4q3EFH38ltMXmVxWHpcJMgqWph1Ls9M\", target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"></a>&nbsp;to run on GPU (Runtime > Change Runtime Type > GPU)</p>\n",
    "\n",
    "The preprocessing and training take a while. Reload cells are provided.\n",
    "\n",
    "Jump to:\n",
    "\n",
    "[Reload preprocessed data](#reload-preprocessed-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_url\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6a579f365d89f2f1\n",
      "Reusing dataset json (C:\\Users\\federico trifoglio\\.cache\\huggingface\\datasets\\json\\default-6a579f365d89f2f1\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3019\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = hf_hub_url(\n",
    "    repo_id=\"lewtun/github-issues\",\n",
    "    filename=\"datasets-issues-with-comments.jsonl\",\n",
    "    repo_type='dataset',\n",
    ")\n",
    "issues_dataset = load_dataset('json', data_files=data_files, split='train')\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "- remove pull requests\n",
    "- remove issues with no replies\n",
    "- remove non-informative columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\federico trifoglio\\.cache\\huggingface\\datasets\\json\\default-6a579f365d89f2f1\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-7adeb9322eeeff4e.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.filter(lambda x: (x['is_pull_request'] == False and len(x['comments']) > 0))\n",
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = ['html_url', 'title', 'comments', 'body']\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example/feature engineering\n",
    "\n",
    "- extract comments from issues\n",
    "- remove comments with less than 15 words\n",
    "- create text feature as title + body + comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comments(examples):\n",
    "  \"\"\"\n",
    "  Extract comments from a single issue\n",
    "  Each nested comment becomes a new example\n",
    "  \"\"\"  \n",
    "  # flatten the comments\n",
    "  results = {'comments': [c for cs in examples['comments'] for c in cs]}\n",
    "  # repeat ['html_url', 'title', 'body'] as many times as the number of comments\n",
    "  for c in ['html_url', 'title', 'body']:\n",
    "    results[c] = [el for n, el in zip([len(cs) for cs in examples['comments']], examples[c]) for _ in range(n)]\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.09ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 44.78ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 42.86ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 23.26ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.map(extract_comments, batched=True)\n",
    "issues_dataset = issues_dataset.map(lambda x: \n",
    "    {'comment_length': \n",
    "        [ len(o.split()) for o in x['comments'] ]\n",
    "    }, batched=True)\n",
    "issues_dataset = issues_dataset.filter(lambda x: x['comment_length'] > 15)\n",
    "issues_dataset = issues_dataset.map(lambda x: \n",
    "    {'text': \n",
    "        [ t+\" \\n \"+b+\" \\n \"+c for t, b, c in zip(x['title'], x['body'], x['comments']) ]\n",
    "    }, batched=True)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SBERT (Sentence-BERT) can be used to calculate sentence embeddings for downstream similiarity tasks.\n",
    "\n",
    "[multi-qa-mpnet-base-dot-v1](https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1) maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search (given a question / search query, these models are able to find relevant text passages). It has been trained on 215M (question, answer) pairs from diverse sources.\n",
    "\n",
    "```\n",
    "query_embedding = model.encode('How big is London')\n",
    "passage_embedding = model.encode(['London has 9,787,426 inhabitants at the 2011 census',\n",
    "                                  'London is known for its finacial district'])\n",
    "\n",
    "print(\"Similarity:\", util.dot_score(query_embedding, passage_embedding))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFMPNetModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "# multi-qa-mpnet-base-dot-v1 has PyTorch weights, \n",
    "# from_pt=True will convert them to the TensorFlow format\n",
    "model = TFAutoModel.from_pretrained(pretrained_model, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    \"\"\"\n",
    "    Collect the last hidden state for the special [CLS] token\n",
    "    In BERT, the final hidden state corresponding to [CLS] token \n",
    "    is used as the aggregate sequence representation for \n",
    "    classification tasks.\n",
    "    \"\"\"\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "def get_embeddings(text):\n",
    "    \"\"\"\n",
    "    Text > Tokenize > Model > CLS Pooling > Numpy of shape (768,)\n",
    "    \"\"\"\n",
    "    encoded_input = tokenizer(\n",
    "        text, padding=True, truncation=True, return_tensors='tf'\n",
    "    )\n",
    "    model_output = model(**encoded_input)\n",
    "    sentence_embedding = cls_pooling(model_output)\n",
    "    return sentence_embedding.numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses, embeddings_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://b8fa87f3-ce62-4d91-a29f-be06f43e260d/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://b8fa87f3-ce62-4d91-a29f-be06f43e260d/assets\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function <lambda> at 0x0000026CF83901F0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 3/3 [56:24<00:00, 1128.21s/ba]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4h 13min 18s\n",
      "Wall time: 57min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embeddings_dataset = issues_dataset.map(lambda x: \n",
    "        {'embeddings': \n",
    "            [ get_embeddings(t) for t in x['text'] ]\n",
    "        }, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_dataset.save_to_disk('faiss-github')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFMPNetModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_from_disk\n",
    "# embeddings_dataset = load_from_disk('faiss-github')\n",
    "# pretrained_model = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "# model = TFAutoModel.from_pretrained(pretrained_model, from_pt=True)\n",
    "# def cls_pooling(model_output):\n",
    "#     \"\"\"\n",
    "#     Collect the last hidden state for the special [CLS] token\n",
    "#     In BERT, the final hidden state corresponding to [CLS] token \n",
    "#     is used as the aggregate sequence representation for \n",
    "#     classification tasks.\n",
    "#     \"\"\"\n",
    "#     return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "# def get_embeddings(text):\n",
    "#     \"\"\"\n",
    "#     Text > Tokenize > Model > CLS Pooling > Numpy of shape (768,)\n",
    "#     \"\"\"\n",
    "#     encoded_input = tokenizer(\n",
    "#         text, padding=True, truncation=True, return_tensors='tf'\n",
    "#     )\n",
    "#     model_output = model(**encoded_input)\n",
    "#     sentence_embedding = cls_pooling(model_output)\n",
    "#     return sentence_embedding.numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea behind FAISS is to create a special data structure called an *index* that allows one to find which embeddings are similar to an input embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 136.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can I load a dataset offline?\"\n",
    "question_embedding = get_embeddings(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if of...</td>\n",
       "      <td>25.505032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package ...</td>\n",
       "      <td>24.555557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I opened a PR that allows to reload modules that have already been loaded once even if there's n...</td>\n",
       "      <td>24.148973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&gt; here is my way to load a dataset offline, but it **requires** an online machine\\n&gt; \\n&gt; 1. (onl...</td>\n",
       "      <td>22.893991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>here is my way to load a dataset offline, but it **requires** an online machine\\r\\n1. (online ma...</td>\n",
       "      <td>22.406647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              comments  \\\n",
       "0  Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if of...   \n",
       "1  The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package ...   \n",
       "2  I opened a PR that allows to reload modules that have already been loaded once even if there's n...   \n",
       "3  > here is my way to load a dataset offline, but it **requires** an online machine\\n> \\n> 1. (onl...   \n",
       "4  here is my way to load a dataset offline, but it **requires** an online machine\\r\\n1. (online ma...   \n",
       "\n",
       "      scores  \n",
       "0  25.505032  \n",
       "1  24.555557  \n",
       "2  24.148973  \n",
       "3  22.893991  \n",
       "4  22.406647  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=5\n",
    ")\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df['scores'] = scores\n",
    "samples_df = samples_df.sort_values('scores', ascending=False).reset_index(drop=True)\n",
    "pd.set_option('max_colwidth', 100)\n",
    "samples_df[['comments', 'scores']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: How can I load a dataset offline?\n",
      "ANSWER: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\n",
      "\n",
      "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n"
     ]
    }
   ],
   "source": [
    "best_answer = samples_df.loc[0, 'comments']\n",
    "print(\"QUESTION:\", question)\n",
    "print(\"ANSWER:\", best_answer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0fedcb30a8eb2d950b9ba0d507fd4e7931c709399d7af09c408a7f0d462a67f2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
